{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    LeakyReLU,\n",
    ")\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 加载MNIST数据集\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 数据预处理\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis=-1)  # 增加通道维度\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 数据增强\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    zoom_range=0.5,\n",
    "    shear_range=0.5,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# 构建 BP 神经网络模型\n",
    "# 没有池化 池化操作主要应用在CNN网络中\n",
    "model = Sequential(\n",
    "    [\n",
    "        Flatten(input_shape=(28, 28, 1)),\n",
    "        \n",
    "        Dense(1024),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(1024),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # ----------------- 两次完全一样的 提取特征 -----------------\n",
    "        \n",
    "        Dense(512),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(128),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(64),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(32),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # 输入结果\n",
    "        Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 编译模型\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), # 使用 Adam 优化器\n",
    "    loss=\"categorical_crossentropy\",  # 多分类交叉熵损失\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# 回调函数\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=10, min_lr=1e-6, verbose=1\n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "model.fit(\n",
    "    datagen.flow(x_train, y_train, batch_size=64),\n",
    "    epochs=300,\n",
    "    validation_data=(x_test, y_test),\n",
    "    steps_per_epoch=len(x_train) // 64,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    ")\n",
    "\n",
    "# 评估模型\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"mnist_model_bp_x4.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BP网络中 反向传播是如何体现的 ?\n",
    "\n",
    "在 Keras 中，反向传播是由框架自动处理的。你不需要显式地编写反向传播的代码。反向传播的过程在模型编译和训练过程中自动完成。以下是代码中体现反向传播的部分：\n",
    "\n",
    "1. **模型编译**：\n",
    "在编译模型时，指定了优化器和损失函数。优化器（如 Adam）会在训练过程中自动执行反向传播，以更新模型的权重。\n",
    "\n",
    "2. **模型训练**：\n",
    "在调用 `model.fit` 方法时，Keras 会自动执行前向传播和反向传播，以最小化损失函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **前向传播**：\n",
    "输入数据通过网络的各层，计算输出和损失。\n",
    "\n",
    "2. **计算损失**：\n",
    "使用指定的损失函数（如 categorical_crossentropy）计算预测值与真实值之间的差异。\n",
    "\n",
    "3. **反向传播**：\n",
    "计算损失相对于每个参数的梯度。\n",
    "优化器（如 Adam）使用这些梯度更新模型的权重。\n",
    "\n",
    "4. **权重更新**：\n",
    "根据计算的梯度和学习率，优化器调整模型的权重，以最小化损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "通过这些步骤，Keras 自动处理反向传播的过程，更新模型的权重，以最小化损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 与卷积神经网络的不同点 ?\n",
    "\n",
    "在传统的 BP 神经网络（全连接层）中，通常不使用池化操作。池化操作主要用于卷积神经网络（CNN）中，用于减少特征图的尺寸和计算量，同时保留重要的特征。\n",
    "\n",
    "### 为什么 BP 网络中不需要池化：\n",
    "\n",
    "1. **全连接层的特点**：\n",
    "全连接层中的每个神经元与前一层的所有神经元相连接，因此不需要通过池化来减少特征图的尺寸。\n",
    "\n",
    "2. **池化的作用**：\n",
    "池化操作（如 MaxPooling）主要用于卷积层之后，用于减少特征图的尺寸，降低计算量，并提高模型的平移不变性。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
