{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e27b176-d698-4e04-8b27-78f53d0265b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-11 20:21:42.787999: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2025-01-11 20:21:42.788035: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-01-11 20:21:42.788047: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-01-11 20:21:42.788076: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-11 20:21:42.788091: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-11 20:21:44.099342: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/469 [============================>.] - ETA: 0s - loss: 2.4503 - accuracy: 0.1533"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-11 20:22:03.997202: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 24s 41ms/step - loss: 2.4498 - accuracy: 0.1535 - val_loss: 1.9828 - val_accuracy: 0.2270 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 2.0875 - accuracy: 0.2257 - val_loss: 1.7165 - val_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.9327 - accuracy: 0.2859 - val_loss: 1.4588 - val_accuracy: 0.4570 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.7616 - accuracy: 0.3671 - val_loss: 1.0941 - val_accuracy: 0.5591 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.5887 - accuracy: 0.4405 - val_loss: 0.8760 - val_accuracy: 0.6948 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.4744 - accuracy: 0.4890 - val_loss: 0.7988 - val_accuracy: 0.7125 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.3739 - accuracy: 0.5285 - val_loss: 0.6243 - val_accuracy: 0.7972 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.3131 - accuracy: 0.5560 - val_loss: 0.5800 - val_accuracy: 0.8257 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.2564 - accuracy: 0.5788 - val_loss: 0.4989 - val_accuracy: 0.8532 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.2096 - accuracy: 0.5987 - val_loss: 0.4844 - val_accuracy: 0.8622 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.1766 - accuracy: 0.6114 - val_loss: 0.4501 - val_accuracy: 0.8704 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.1362 - accuracy: 0.6267 - val_loss: 0.4060 - val_accuracy: 0.8934 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.1039 - accuracy: 0.6397 - val_loss: 0.3736 - val_accuracy: 0.9068 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.0759 - accuracy: 0.6531 - val_loss: 0.3565 - val_accuracy: 0.8924 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.0411 - accuracy: 0.6637 - val_loss: 0.3109 - val_accuracy: 0.9179 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 1.0250 - accuracy: 0.6703 - val_loss: 0.2925 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.9976 - accuracy: 0.6800 - val_loss: 0.2835 - val_accuracy: 0.9236 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.9770 - accuracy: 0.6863 - val_loss: 0.2525 - val_accuracy: 0.9333 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.9595 - accuracy: 0.6920 - val_loss: 0.2721 - val_accuracy: 0.9238 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.9353 - accuracy: 0.7025 - val_loss: 0.2338 - val_accuracy: 0.9341 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.9184 - accuracy: 0.7078 - val_loss: 0.2388 - val_accuracy: 0.9318 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.9085 - accuracy: 0.7126 - val_loss: 0.2240 - val_accuracy: 0.9361 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.8942 - accuracy: 0.7165 - val_loss: 0.2000 - val_accuracy: 0.9426 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.8810 - accuracy: 0.7207 - val_loss: 0.1942 - val_accuracy: 0.9451 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.8707 - accuracy: 0.7238 - val_loss: 0.1945 - val_accuracy: 0.9450 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.8574 - accuracy: 0.7287 - val_loss: 0.2229 - val_accuracy: 0.9332 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.8438 - accuracy: 0.7337 - val_loss: 0.1754 - val_accuracy: 0.9510 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.8310 - accuracy: 0.7365 - val_loss: 0.1813 - val_accuracy: 0.9469 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.8226 - accuracy: 0.7403 - val_loss: 0.1830 - val_accuracy: 0.9472 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.8187 - accuracy: 0.7412 - val_loss: 0.1623 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.8018 - accuracy: 0.7455 - val_loss: 0.1660 - val_accuracy: 0.9502 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.8047 - accuracy: 0.7449 - val_loss: 0.1634 - val_accuracy: 0.9505 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.7879 - accuracy: 0.7504 - val_loss: 0.1441 - val_accuracy: 0.9590 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.7841 - accuracy: 0.7517 - val_loss: 0.1495 - val_accuracy: 0.9556 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.7725 - accuracy: 0.7548 - val_loss: 0.1516 - val_accuracy: 0.9557 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "469/469 [==============================] - 957s 2s/step - loss: 0.7652 - accuracy: 0.7584 - val_loss: 0.1554 - val_accuracy: 0.9537 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "469/469 [==============================] - 299s 639ms/step - loss: 0.7526 - accuracy: 0.7604 - val_loss: 0.1442 - val_accuracy: 0.9572 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.7531 - accuracy: 0.7616 - val_loss: 0.1400 - val_accuracy: 0.9575 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.7434 - accuracy: 0.7652 - val_loss: 0.1529 - val_accuracy: 0.9538 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.7520 - accuracy: 0.7624 - val_loss: 0.1304 - val_accuracy: 0.9607 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.7289 - accuracy: 0.7684 - val_loss: 0.1311 - val_accuracy: 0.9610 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.7360 - accuracy: 0.7686 - val_loss: 0.1372 - val_accuracy: 0.9606 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.7284 - accuracy: 0.7693 - val_loss: 0.1296 - val_accuracy: 0.9609 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.7170 - accuracy: 0.7720 - val_loss: 0.1386 - val_accuracy: 0.9572 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.7232 - accuracy: 0.7701 - val_loss: 0.1299 - val_accuracy: 0.9614 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.7163 - accuracy: 0.7733 - val_loss: 0.1370 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.7071 - accuracy: 0.7754 - val_loss: 0.1338 - val_accuracy: 0.9580 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.7076 - accuracy: 0.7769 - val_loss: 0.1301 - val_accuracy: 0.9605 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.7022 - accuracy: 0.7778 - val_loss: 0.1294 - val_accuracy: 0.9597 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6913 - accuracy: 0.7817 - val_loss: 0.1331 - val_accuracy: 0.9596 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6844 - accuracy: 0.7834 - val_loss: 0.1177 - val_accuracy: 0.9640 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6900 - accuracy: 0.7819 - val_loss: 0.1323 - val_accuracy: 0.9591 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6832 - accuracy: 0.7847 - val_loss: 0.1183 - val_accuracy: 0.9660 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6781 - accuracy: 0.7864 - val_loss: 0.1221 - val_accuracy: 0.9622 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6813 - accuracy: 0.7842 - val_loss: 0.1225 - val_accuracy: 0.9627 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6648 - accuracy: 0.7901 - val_loss: 0.1188 - val_accuracy: 0.9634 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6683 - accuracy: 0.7889 - val_loss: 0.1132 - val_accuracy: 0.9653 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6542 - accuracy: 0.7938 - val_loss: 0.1084 - val_accuracy: 0.9669 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6644 - accuracy: 0.7912 - val_loss: 0.1074 - val_accuracy: 0.9665 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6620 - accuracy: 0.7916 - val_loss: 0.1141 - val_accuracy: 0.9660 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6507 - accuracy: 0.7942 - val_loss: 0.1140 - val_accuracy: 0.9659 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6491 - accuracy: 0.7949 - val_loss: 0.1221 - val_accuracy: 0.9638 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6494 - accuracy: 0.7961 - val_loss: 0.1096 - val_accuracy: 0.9676 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6533 - accuracy: 0.7940 - val_loss: 0.1130 - val_accuracy: 0.9654 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6441 - accuracy: 0.7969 - val_loss: 0.1163 - val_accuracy: 0.9658 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6325 - accuracy: 0.7995 - val_loss: 0.1042 - val_accuracy: 0.9681 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6412 - accuracy: 0.7960 - val_loss: 0.1140 - val_accuracy: 0.9667 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6294 - accuracy: 0.8023 - val_loss: 0.1141 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6333 - accuracy: 0.8011 - val_loss: 0.1197 - val_accuracy: 0.9636 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6311 - accuracy: 0.8017 - val_loss: 0.1130 - val_accuracy: 0.9682 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6262 - accuracy: 0.8020 - val_loss: 0.1112 - val_accuracy: 0.9664 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6205 - accuracy: 0.8043 - val_loss: 0.1068 - val_accuracy: 0.9683 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6203 - accuracy: 0.8043 - val_loss: 0.1145 - val_accuracy: 0.9652 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.6169 - accuracy: 0.8043\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.6170 - accuracy: 0.8043 - val_loss: 0.1121 - val_accuracy: 0.9656 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5905 - accuracy: 0.8155 - val_loss: 0.0985 - val_accuracy: 0.9703 - lr: 5.0000e-04\n",
      "Epoch 76/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5833 - accuracy: 0.8153 - val_loss: 0.0998 - val_accuracy: 0.9702 - lr: 5.0000e-04\n",
      "Epoch 77/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5805 - accuracy: 0.8166 - val_loss: 0.0974 - val_accuracy: 0.9703 - lr: 5.0000e-04\n",
      "Epoch 78/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5755 - accuracy: 0.8192 - val_loss: 0.0968 - val_accuracy: 0.9715 - lr: 5.0000e-04\n",
      "Epoch 79/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5661 - accuracy: 0.8213 - val_loss: 0.0987 - val_accuracy: 0.9707 - lr: 5.0000e-04\n",
      "Epoch 80/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5552 - accuracy: 0.8249 - val_loss: 0.0906 - val_accuracy: 0.9730 - lr: 5.0000e-04\n",
      "Epoch 81/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5586 - accuracy: 0.8235 - val_loss: 0.0946 - val_accuracy: 0.9711 - lr: 5.0000e-04\n",
      "Epoch 82/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5622 - accuracy: 0.8214 - val_loss: 0.0979 - val_accuracy: 0.9706 - lr: 5.0000e-04\n",
      "Epoch 83/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5514 - accuracy: 0.8270 - val_loss: 0.0939 - val_accuracy: 0.9716 - lr: 5.0000e-04\n",
      "Epoch 84/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5521 - accuracy: 0.8234 - val_loss: 0.0915 - val_accuracy: 0.9715 - lr: 5.0000e-04\n",
      "Epoch 85/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5493 - accuracy: 0.8242 - val_loss: 0.0968 - val_accuracy: 0.9701 - lr: 5.0000e-04\n",
      "Epoch 86/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5474 - accuracy: 0.8279 - val_loss: 0.0929 - val_accuracy: 0.9707 - lr: 5.0000e-04\n",
      "Epoch 87/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5494 - accuracy: 0.8254 - val_loss: 0.0950 - val_accuracy: 0.9691 - lr: 5.0000e-04\n",
      "Epoch 88/200\n",
      "469/469 [==============================] - ETA: 0s - loss: 0.5467 - accuracy: 0.8280\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5467 - accuracy: 0.8280 - val_loss: 0.0926 - val_accuracy: 0.9717 - lr: 5.0000e-04\n",
      "Epoch 89/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5397 - accuracy: 0.8310 - val_loss: 0.0918 - val_accuracy: 0.9727 - lr: 2.5000e-04\n",
      "Epoch 90/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5244 - accuracy: 0.8338 - val_loss: 0.0906 - val_accuracy: 0.9716 - lr: 2.5000e-04\n",
      "Epoch 91/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5211 - accuracy: 0.8354 - val_loss: 0.0863 - val_accuracy: 0.9739 - lr: 2.5000e-04\n",
      "Epoch 92/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5305 - accuracy: 0.8319 - val_loss: 0.0875 - val_accuracy: 0.9734 - lr: 2.5000e-04\n",
      "Epoch 93/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5216 - accuracy: 0.8334 - val_loss: 0.0877 - val_accuracy: 0.9725 - lr: 2.5000e-04\n",
      "Epoch 94/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5219 - accuracy: 0.8359 - val_loss: 0.0841 - val_accuracy: 0.9742 - lr: 2.5000e-04\n",
      "Epoch 95/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5226 - accuracy: 0.8346 - val_loss: 0.0844 - val_accuracy: 0.9742 - lr: 2.5000e-04\n",
      "Epoch 96/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5165 - accuracy: 0.8367 - val_loss: 0.0839 - val_accuracy: 0.9751 - lr: 2.5000e-04\n",
      "Epoch 97/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5234 - accuracy: 0.8344 - val_loss: 0.0841 - val_accuracy: 0.9740 - lr: 2.5000e-04\n",
      "Epoch 98/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5145 - accuracy: 0.8392 - val_loss: 0.0854 - val_accuracy: 0.9733 - lr: 2.5000e-04\n",
      "Epoch 99/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5145 - accuracy: 0.8375 - val_loss: 0.0891 - val_accuracy: 0.9725 - lr: 2.5000e-04\n",
      "Epoch 100/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5041 - accuracy: 0.8405 - val_loss: 0.0857 - val_accuracy: 0.9733 - lr: 2.5000e-04\n",
      "Epoch 101/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5165 - accuracy: 0.8362 - val_loss: 0.0867 - val_accuracy: 0.9727 - lr: 2.5000e-04\n",
      "Epoch 102/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5058 - accuracy: 0.8385 - val_loss: 0.0827 - val_accuracy: 0.9749 - lr: 2.5000e-04\n",
      "Epoch 103/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5153 - accuracy: 0.8374 - val_loss: 0.0865 - val_accuracy: 0.9732 - lr: 2.5000e-04\n",
      "Epoch 104/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5029 - accuracy: 0.8429 - val_loss: 0.0838 - val_accuracy: 0.9733 - lr: 2.5000e-04\n",
      "Epoch 105/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5020 - accuracy: 0.8421 - val_loss: 0.0831 - val_accuracy: 0.9747 - lr: 2.5000e-04\n",
      "Epoch 106/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5056 - accuracy: 0.8399 - val_loss: 0.0808 - val_accuracy: 0.9741 - lr: 2.5000e-04\n",
      "Epoch 107/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5051 - accuracy: 0.8405 - val_loss: 0.0834 - val_accuracy: 0.9739 - lr: 2.5000e-04\n",
      "Epoch 108/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5044 - accuracy: 0.8403 - val_loss: 0.0793 - val_accuracy: 0.9754 - lr: 2.5000e-04\n",
      "Epoch 109/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5030 - accuracy: 0.8395 - val_loss: 0.0811 - val_accuracy: 0.9756 - lr: 2.5000e-04\n",
      "Epoch 110/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5053 - accuracy: 0.8407 - val_loss: 0.0808 - val_accuracy: 0.9758 - lr: 2.5000e-04\n",
      "Epoch 111/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4945 - accuracy: 0.8457 - val_loss: 0.0810 - val_accuracy: 0.9756 - lr: 2.5000e-04\n",
      "Epoch 112/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5022 - accuracy: 0.8418 - val_loss: 0.0832 - val_accuracy: 0.9739 - lr: 2.5000e-04\n",
      "Epoch 113/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4987 - accuracy: 0.8443 - val_loss: 0.0880 - val_accuracy: 0.9732 - lr: 2.5000e-04\n",
      "Epoch 114/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4994 - accuracy: 0.8420 - val_loss: 0.0834 - val_accuracy: 0.9746 - lr: 2.5000e-04\n",
      "Epoch 115/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4979 - accuracy: 0.8423 - val_loss: 0.0811 - val_accuracy: 0.9754 - lr: 2.5000e-04\n",
      "Epoch 116/200\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.5044 - accuracy: 0.8408\n",
      "Epoch 116: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5047 - accuracy: 0.8407 - val_loss: 0.0812 - val_accuracy: 0.9758 - lr: 2.5000e-04\n",
      "Epoch 117/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.5016 - accuracy: 0.8407 - val_loss: 0.0807 - val_accuracy: 0.9755 - lr: 1.2500e-04\n",
      "Epoch 118/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4927 - accuracy: 0.8439 - val_loss: 0.0795 - val_accuracy: 0.9759 - lr: 1.2500e-04\n",
      "Epoch 119/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4987 - accuracy: 0.8429 - val_loss: 0.0812 - val_accuracy: 0.9750 - lr: 1.2500e-04\n",
      "Epoch 120/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4898 - accuracy: 0.8455 - val_loss: 0.0785 - val_accuracy: 0.9755 - lr: 1.2500e-04\n",
      "Epoch 121/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4879 - accuracy: 0.8465 - val_loss: 0.0807 - val_accuracy: 0.9751 - lr: 1.2500e-04\n",
      "Epoch 122/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4848 - accuracy: 0.8472 - val_loss: 0.0806 - val_accuracy: 0.9747 - lr: 1.2500e-04\n",
      "Epoch 123/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4898 - accuracy: 0.8447 - val_loss: 0.0808 - val_accuracy: 0.9749 - lr: 1.2500e-04\n",
      "Epoch 124/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4872 - accuracy: 0.8457 - val_loss: 0.0815 - val_accuracy: 0.9744 - lr: 1.2500e-04\n",
      "Epoch 125/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4915 - accuracy: 0.8449 - val_loss: 0.0802 - val_accuracy: 0.9749 - lr: 1.2500e-04\n",
      "Epoch 126/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4854 - accuracy: 0.8478 - val_loss: 0.0788 - val_accuracy: 0.9748 - lr: 1.2500e-04\n",
      "Epoch 127/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4849 - accuracy: 0.8473 - val_loss: 0.0782 - val_accuracy: 0.9747 - lr: 1.2500e-04\n",
      "Epoch 128/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4830 - accuracy: 0.8484 - val_loss: 0.0830 - val_accuracy: 0.9741 - lr: 1.2500e-04\n",
      "Epoch 129/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4831 - accuracy: 0.8471 - val_loss: 0.0770 - val_accuracy: 0.9755 - lr: 1.2500e-04\n",
      "Epoch 130/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4830 - accuracy: 0.8465 - val_loss: 0.0795 - val_accuracy: 0.9753 - lr: 1.2500e-04\n",
      "Epoch 131/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4826 - accuracy: 0.8474 - val_loss: 0.0785 - val_accuracy: 0.9758 - lr: 1.2500e-04\n",
      "Epoch 132/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4874 - accuracy: 0.8449 - val_loss: 0.0800 - val_accuracy: 0.9752 - lr: 1.2500e-04\n",
      "Epoch 133/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4877 - accuracy: 0.8457 - val_loss: 0.0784 - val_accuracy: 0.9763 - lr: 1.2500e-04\n",
      "Epoch 134/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4777 - accuracy: 0.8488 - val_loss: 0.0761 - val_accuracy: 0.9764 - lr: 1.2500e-04\n",
      "Epoch 135/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4762 - accuracy: 0.8506 - val_loss: 0.0788 - val_accuracy: 0.9757 - lr: 1.2500e-04\n",
      "Epoch 136/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4756 - accuracy: 0.8509 - val_loss: 0.0788 - val_accuracy: 0.9758 - lr: 1.2500e-04\n",
      "Epoch 137/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4801 - accuracy: 0.8485 - val_loss: 0.0787 - val_accuracy: 0.9752 - lr: 1.2500e-04\n",
      "Epoch 138/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4828 - accuracy: 0.8474 - val_loss: 0.0795 - val_accuracy: 0.9744 - lr: 1.2500e-04\n",
      "Epoch 139/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4828 - accuracy: 0.8479 - val_loss: 0.0798 - val_accuracy: 0.9742 - lr: 1.2500e-04\n",
      "Epoch 140/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4754 - accuracy: 0.8508 - val_loss: 0.0807 - val_accuracy: 0.9740 - lr: 1.2500e-04\n",
      "Epoch 141/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4809 - accuracy: 0.8481 - val_loss: 0.0778 - val_accuracy: 0.9756 - lr: 1.2500e-04\n",
      "Epoch 142/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4743 - accuracy: 0.8521 - val_loss: 0.0756 - val_accuracy: 0.9765 - lr: 1.2500e-04\n",
      "Epoch 143/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4691 - accuracy: 0.8528 - val_loss: 0.0742 - val_accuracy: 0.9768 - lr: 1.2500e-04\n",
      "Epoch 144/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4735 - accuracy: 0.8497 - val_loss: 0.0769 - val_accuracy: 0.9766 - lr: 1.2500e-04\n",
      "Epoch 145/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4703 - accuracy: 0.8514 - val_loss: 0.0782 - val_accuracy: 0.9756 - lr: 1.2500e-04\n",
      "Epoch 146/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4762 - accuracy: 0.8499 - val_loss: 0.0789 - val_accuracy: 0.9761 - lr: 1.2500e-04\n",
      "Epoch 147/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4774 - accuracy: 0.8482 - val_loss: 0.0816 - val_accuracy: 0.9748 - lr: 1.2500e-04\n",
      "Epoch 148/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4710 - accuracy: 0.8513 - val_loss: 0.0791 - val_accuracy: 0.9755 - lr: 1.2500e-04\n",
      "Epoch 149/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4720 - accuracy: 0.8515 - val_loss: 0.0754 - val_accuracy: 0.9765 - lr: 1.2500e-04\n",
      "Epoch 150/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4720 - accuracy: 0.8514 - val_loss: 0.0773 - val_accuracy: 0.9753 - lr: 1.2500e-04\n",
      "Epoch 151/200\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.4700 - accuracy: 0.8524\n",
      "Epoch 151: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4698 - accuracy: 0.8525 - val_loss: 0.0758 - val_accuracy: 0.9755 - lr: 1.2500e-04\n",
      "Epoch 152/200\n",
      "469/469 [==============================] - 117s 249ms/step - loss: 0.4753 - accuracy: 0.8505 - val_loss: 0.0747 - val_accuracy: 0.9763 - lr: 6.2500e-05\n",
      "Epoch 153/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4746 - accuracy: 0.8494 - val_loss: 0.0749 - val_accuracy: 0.9771 - lr: 6.2500e-05\n",
      "Epoch 154/200\n",
      "469/469 [==============================] - 18s 39ms/step - loss: 0.4751 - accuracy: 0.8511 - val_loss: 0.0757 - val_accuracy: 0.9764 - lr: 6.2500e-05\n",
      "Epoch 155/200\n",
      "469/469 [==============================] - 18s 39ms/step - loss: 0.4737 - accuracy: 0.8509 - val_loss: 0.0760 - val_accuracy: 0.9764 - lr: 6.2500e-05\n",
      "Epoch 156/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4674 - accuracy: 0.8514 - val_loss: 0.0747 - val_accuracy: 0.9767 - lr: 6.2500e-05\n",
      "Epoch 157/200\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4705 - accuracy: 0.8523 - val_loss: 0.0763 - val_accuracy: 0.9765 - lr: 6.2500e-05\n",
      "Epoch 158/200\n",
      "468/469 [============================>.] - ETA: 0s - loss: 0.4580 - accuracy: 0.8554Restoring model weights from the end of the best epoch: 143.\n",
      "469/469 [==============================] - 18s 38ms/step - loss: 0.4582 - accuracy: 0.8553 - val_loss: 0.0756 - val_accuracy: 0.9760 - lr: 6.2500e-05\n",
      "Epoch 158: early stopping\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.0742 - accuracy: 0.9768\n",
      "Test accuracy: 0.9768000245094299\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    Input,\n",
    "    LeakyReLU,\n",
    ")\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# 加载MNIST数据集\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 数据预处理：归一化\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)  # 调整形状以匹配模型输入\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 数据增强\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    zoom_range=0.5,\n",
    "    shear_range=0.5,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "datagen.fit(x_train.reshape(-1, 28, 28, 1))  # 数据增强需要原始形状\n",
    "\n",
    "# 构建多层感知机（MLP）神经网络模型\n",
    "model = Sequential(\n",
    "    [\n",
    "        Input(shape=(28, 28, 1)),  # 输入层\n",
    "        \n",
    "        tf.keras.layers.Flatten(),  # 展平图像\n",
    "        Dense(2048),  # 隐藏层 1\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(1024),  # 隐藏层 2\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(512),  # 隐藏层 3\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(256),  # 隐藏层 4\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(128),  # 隐藏层 5\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(64),  # 隐藏层 6\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(32),  # 隐藏层 7\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        Dense(10, activation=\"softmax\"),  # 输出层\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 编译模型\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),  # 使用RMSprop优化器\n",
    "    loss=\"categorical_crossentropy\",  # 多分类交叉熵损失\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# 回调函数\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=8, min_lr=1e-6, verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=1\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(\n",
    "    datagen.flow(x_train.reshape(-1, 28, 28, 1), y_train, batch_size=128),  # 调整批量大小并恢复原始形状\n",
    "    epochs=200,  # 增加训练轮数\n",
    "    validation_data=(x_test.reshape(-1, 28, 28, 1), y_test),  # 恢复原始形状\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    ")\n",
    "\n",
    "# 评估模型\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)  # 使用调整后的测试数据形状\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"mnist_model_mlp_v2.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
